LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForCausalLM                                        --
├─LlamaModel: 1-1                                       --
│    └─Embedding: 2-1                                   262,668,288
│    └─ModuleList: 2-2                                  --
│    │    └─LlamaDecoderLayer: 3-1                      60,821,504
│    │    └─LlamaDecoderLayer: 3-2                      60,821,504
│    │    └─LlamaDecoderLayer: 3-3                      60,821,504
│    │    └─LlamaDecoderLayer: 3-4                      60,821,504
│    │    └─LlamaDecoderLayer: 3-5                      60,821,504
│    │    └─LlamaDecoderLayer: 3-6                      60,821,504
│    │    └─LlamaDecoderLayer: 3-7                      60,821,504
│    │    └─LlamaDecoderLayer: 3-8                      60,821,504
│    │    └─LlamaDecoderLayer: 3-9                      60,821,504
│    │    └─LlamaDecoderLayer: 3-10                     60,821,504
│    │    └─LlamaDecoderLayer: 3-11                     60,821,504
│    │    └─LlamaDecoderLayer: 3-12                     60,821,504
│    │    └─LlamaDecoderLayer: 3-13                     60,821,504
│    │    └─LlamaDecoderLayer: 3-14                     60,821,504
│    │    └─LlamaDecoderLayer: 3-15                     60,821,504
│    │    └─LlamaDecoderLayer: 3-16                     60,821,504
│    └─LlamaRMSNorm: 2-3                                2,048
│    └─LlamaRotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           262,668,288
================================================================================
Total params: 1,498,482,688
Trainable params: 1,498,482,688
Non-trainable params: 0
================================================================================================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForCausalLM                                        --
├─LlamaModel: 1-1                                       --
│    └─embed_tokens.weight                              ├─262,668,288
│    └─layers.0.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.0.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.0.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.0.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.0.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.0.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.0.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.0.input_layernorm.weight                  ├─2,048
│    └─layers.0.post_attention_layernorm.weight         ├─2,048
│    └─layers.1.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.1.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.1.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.1.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.1.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.1.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.1.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.1.input_layernorm.weight                  ├─2,048
│    └─layers.1.post_attention_layernorm.weight         ├─2,048
│    └─layers.2.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.2.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.2.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.2.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.2.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.2.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.2.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.2.input_layernorm.weight                  ├─2,048
│    └─layers.2.post_attention_layernorm.weight         ├─2,048
│    └─layers.3.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.3.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.3.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.3.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.3.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.3.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.3.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.3.input_layernorm.weight                  ├─2,048
│    └─layers.3.post_attention_layernorm.weight         ├─2,048
│    └─layers.4.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.4.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.4.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.4.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.4.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.4.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.4.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.4.input_layernorm.weight                  ├─2,048
│    └─layers.4.post_attention_layernorm.weight         ├─2,048
│    └─layers.5.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.5.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.5.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.5.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.5.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.5.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.5.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.5.input_layernorm.weight                  ├─2,048
│    └─layers.5.post_attention_layernorm.weight         ├─2,048
│    └─layers.6.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.6.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.6.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.6.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.6.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.6.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.6.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.6.input_layernorm.weight                  ├─2,048
│    └─layers.6.post_attention_layernorm.weight         ├─2,048
│    └─layers.7.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.7.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.7.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.7.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.7.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.7.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.7.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.7.input_layernorm.weight                  ├─2,048
│    └─layers.7.post_attention_layernorm.weight         ├─2,048
│    └─layers.8.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.8.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.8.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.8.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.8.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.8.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.8.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.8.input_layernorm.weight                  ├─2,048
│    └─layers.8.post_attention_layernorm.weight         ├─2,048
│    └─layers.9.self_attn.q_proj.weight                 ├─4,194,304
│    └─layers.9.self_attn.k_proj.weight                 ├─1,048,576
│    └─layers.9.self_attn.v_proj.weight                 ├─1,048,576
│    └─layers.9.self_attn.o_proj.weight                 ├─4,194,304
│    └─layers.9.mlp.gate_proj.weight                    ├─16,777,216
│    └─layers.9.mlp.up_proj.weight                      ├─16,777,216
│    └─layers.9.mlp.down_proj.weight                    ├─16,777,216
│    └─layers.9.input_layernorm.weight                  ├─2,048
│    └─layers.9.post_attention_layernorm.weight         ├─2,048
│    └─layers.10.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.10.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.10.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.10.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.10.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.10.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.10.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.10.input_layernorm.weight                 ├─2,048
│    └─layers.10.post_attention_layernorm.weight        ├─2,048
│    └─layers.11.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.11.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.11.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.11.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.11.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.11.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.11.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.11.input_layernorm.weight                 ├─2,048
│    └─layers.11.post_attention_layernorm.weight        ├─2,048
│    └─layers.12.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.12.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.12.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.12.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.12.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.12.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.12.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.12.input_layernorm.weight                 ├─2,048
│    └─layers.12.post_attention_layernorm.weight        ├─2,048
│    └─layers.13.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.13.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.13.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.13.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.13.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.13.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.13.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.13.input_layernorm.weight                 ├─2,048
│    └─layers.13.post_attention_layernorm.weight        ├─2,048
│    └─layers.14.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.14.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.14.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.14.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.14.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.14.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.14.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.14.input_layernorm.weight                 ├─2,048
│    └─layers.14.post_attention_layernorm.weight        ├─2,048
│    └─layers.15.self_attn.q_proj.weight                ├─4,194,304
│    └─layers.15.self_attn.k_proj.weight                ├─1,048,576
│    └─layers.15.self_attn.v_proj.weight                ├─1,048,576
│    └─layers.15.self_attn.o_proj.weight                ├─4,194,304
│    └─layers.15.mlp.gate_proj.weight                   ├─16,777,216
│    └─layers.15.mlp.up_proj.weight                     ├─16,777,216
│    └─layers.15.mlp.down_proj.weight                   ├─16,777,216
│    └─layers.15.input_layernorm.weight                 ├─2,048
│    └─layers.15.post_attention_layernorm.weight        ├─2,048
│    └─norm.weight                                      └─2,048
│    └─Embedding: 2-1                                   262,668,288
│    │    └─weight                                      └─262,668,288
│    └─ModuleList: 2-2                                  --
│    │    └─0.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─0.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─0.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─0.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─0.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─0.mlp.up_proj.weight                        ├─16,777,216
│    │    └─0.mlp.down_proj.weight                      ├─16,777,216
│    │    └─0.input_layernorm.weight                    ├─2,048
│    │    └─0.post_attention_layernorm.weight           ├─2,048
│    │    └─1.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─1.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─1.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─1.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─1.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─1.mlp.up_proj.weight                        ├─16,777,216
│    │    └─1.mlp.down_proj.weight                      ├─16,777,216
│    │    └─1.input_layernorm.weight                    ├─2,048
│    │    └─1.post_attention_layernorm.weight           ├─2,048
│    │    └─2.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─2.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─2.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─2.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─2.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─2.mlp.up_proj.weight                        ├─16,777,216
│    │    └─2.mlp.down_proj.weight                      ├─16,777,216
│    │    └─2.input_layernorm.weight                    ├─2,048
│    │    └─2.post_attention_layernorm.weight           ├─2,048
│    │    └─3.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─3.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─3.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─3.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─3.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─3.mlp.up_proj.weight                        ├─16,777,216
│    │    └─3.mlp.down_proj.weight                      ├─16,777,216
│    │    └─3.input_layernorm.weight                    ├─2,048
│    │    └─3.post_attention_layernorm.weight           ├─2,048
│    │    └─4.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─4.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─4.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─4.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─4.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─4.mlp.up_proj.weight                        ├─16,777,216
│    │    └─4.mlp.down_proj.weight                      ├─16,777,216
│    │    └─4.input_layernorm.weight                    ├─2,048
│    │    └─4.post_attention_layernorm.weight           ├─2,048
│    │    └─5.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─5.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─5.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─5.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─5.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─5.mlp.up_proj.weight                        ├─16,777,216
│    │    └─5.mlp.down_proj.weight                      ├─16,777,216
│    │    └─5.input_layernorm.weight                    ├─2,048
│    │    └─5.post_attention_layernorm.weight           ├─2,048
│    │    └─6.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─6.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─6.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─6.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─6.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─6.mlp.up_proj.weight                        ├─16,777,216
│    │    └─6.mlp.down_proj.weight                      ├─16,777,216
│    │    └─6.input_layernorm.weight                    ├─2,048
│    │    └─6.post_attention_layernorm.weight           ├─2,048
│    │    └─7.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─7.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─7.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─7.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─7.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─7.mlp.up_proj.weight                        ├─16,777,216
│    │    └─7.mlp.down_proj.weight                      ├─16,777,216
│    │    └─7.input_layernorm.weight                    ├─2,048
│    │    └─7.post_attention_layernorm.weight           ├─2,048
│    │    └─8.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─8.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─8.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─8.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─8.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─8.mlp.up_proj.weight                        ├─16,777,216
│    │    └─8.mlp.down_proj.weight                      ├─16,777,216
│    │    └─8.input_layernorm.weight                    ├─2,048
│    │    └─8.post_attention_layernorm.weight           ├─2,048
│    │    └─9.self_attn.q_proj.weight                   ├─4,194,304
│    │    └─9.self_attn.k_proj.weight                   ├─1,048,576
│    │    └─9.self_attn.v_proj.weight                   ├─1,048,576
│    │    └─9.self_attn.o_proj.weight                   ├─4,194,304
│    │    └─9.mlp.gate_proj.weight                      ├─16,777,216
│    │    └─9.mlp.up_proj.weight                        ├─16,777,216
│    │    └─9.mlp.down_proj.weight                      ├─16,777,216
│    │    └─9.input_layernorm.weight                    ├─2,048
│    │    └─9.post_attention_layernorm.weight           ├─2,048
│    │    └─10.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─10.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─10.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─10.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─10.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─10.mlp.up_proj.weight                       ├─16,777,216
│    │    └─10.mlp.down_proj.weight                     ├─16,777,216
│    │    └─10.input_layernorm.weight                   ├─2,048
│    │    └─10.post_attention_layernorm.weight          ├─2,048
│    │    └─11.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─11.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─11.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─11.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─11.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─11.mlp.up_proj.weight                       ├─16,777,216
│    │    └─11.mlp.down_proj.weight                     ├─16,777,216
│    │    └─11.input_layernorm.weight                   ├─2,048
│    │    └─11.post_attention_layernorm.weight          ├─2,048
│    │    └─12.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─12.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─12.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─12.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─12.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─12.mlp.up_proj.weight                       ├─16,777,216
│    │    └─12.mlp.down_proj.weight                     ├─16,777,216
│    │    └─12.input_layernorm.weight                   ├─2,048
│    │    └─12.post_attention_layernorm.weight          ├─2,048
│    │    └─13.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─13.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─13.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─13.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─13.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─13.mlp.up_proj.weight                       ├─16,777,216
│    │    └─13.mlp.down_proj.weight                     ├─16,777,216
│    │    └─13.input_layernorm.weight                   ├─2,048
│    │    └─13.post_attention_layernorm.weight          ├─2,048
│    │    └─14.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─14.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─14.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─14.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─14.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─14.mlp.up_proj.weight                       ├─16,777,216
│    │    └─14.mlp.down_proj.weight                     ├─16,777,216
│    │    └─14.input_layernorm.weight                   ├─2,048
│    │    └─14.post_attention_layernorm.weight          ├─2,048
│    │    └─15.self_attn.q_proj.weight                  ├─4,194,304
│    │    └─15.self_attn.k_proj.weight                  ├─1,048,576
│    │    └─15.self_attn.v_proj.weight                  ├─1,048,576
│    │    └─15.self_attn.o_proj.weight                  ├─4,194,304
│    │    └─15.mlp.gate_proj.weight                     ├─16,777,216
│    │    └─15.mlp.up_proj.weight                       ├─16,777,216
│    │    └─15.mlp.down_proj.weight                     ├─16,777,216
│    │    └─15.input_layernorm.weight                   ├─2,048
│    │    └─15.post_attention_layernorm.weight          └─2,048
│    │    └─LlamaDecoderLayer: 3-1                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-2                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-3                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-4                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-5                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-6                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-7                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-8                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-9                      60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-10                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-11                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-12                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-13                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-14                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-15                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    │    └─LlamaDecoderLayer: 3-16                     60,821,504
│    │    │    └─self_attn.q_proj.weight                ├─4,194,304
│    │    │    └─self_attn.k_proj.weight                ├─1,048,576
│    │    │    └─self_attn.v_proj.weight                ├─1,048,576
│    │    │    └─self_attn.o_proj.weight                ├─4,194,304
│    │    │    └─mlp.gate_proj.weight                   ├─16,777,216
│    │    │    └─mlp.up_proj.weight                     ├─16,777,216
│    │    │    └─mlp.down_proj.weight                   ├─16,777,216
│    │    │    └─input_layernorm.weight                 ├─2,048
│    │    │    └─post_attention_layernorm.weight        └─2,048
│    └─LlamaRMSNorm: 2-3                                2,048
│    │    └─weight                                      └─2,048
│    └─LlamaRotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           262,668,288
│    └─weight                                           └─262,668,288
================================================================================
Total params: 1,498,482,688
Trainable params: 1,498,482,688
Non-trainable params: 0
================================================================================
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForCausalLM                                        --
├─LlamaModel: 1-1                                       --
│    └─Embedding: 2-1                                   394,002,432
│    └─ModuleList: 2-2                                  --
│    │    └─LlamaDecoderLayer: 3-1                      100,669,440
│    │    └─LlamaDecoderLayer: 3-2                      100,669,440
│    │    └─LlamaDecoderLayer: 3-3                      100,669,440
│    │    └─LlamaDecoderLayer: 3-4                      100,669,440
│    │    └─LlamaDecoderLayer: 3-5                      100,669,440
│    │    └─LlamaDecoderLayer: 3-6                      100,669,440
│    │    └─LlamaDecoderLayer: 3-7                      100,669,440
│    │    └─LlamaDecoderLayer: 3-8                      100,669,440
│    │    └─LlamaDecoderLayer: 3-9                      100,669,440
│    │    └─LlamaDecoderLayer: 3-10                     100,669,440
│    │    └─LlamaDecoderLayer: 3-11                     100,669,440
│    │    └─LlamaDecoderLayer: 3-12                     100,669,440
│    │    └─LlamaDecoderLayer: 3-13                     100,669,440
│    │    └─LlamaDecoderLayer: 3-14                     100,669,440
│    │    └─LlamaDecoderLayer: 3-15                     100,669,440
│    │    └─LlamaDecoderLayer: 3-16                     100,669,440
│    │    └─LlamaDecoderLayer: 3-17                     100,669,440
│    │    └─LlamaDecoderLayer: 3-18                     100,669,440
│    │    └─LlamaDecoderLayer: 3-19                     100,669,440
│    │    └─LlamaDecoderLayer: 3-20                     100,669,440
│    │    └─LlamaDecoderLayer: 3-21                     100,669,440
│    │    └─LlamaDecoderLayer: 3-22                     100,669,440
│    │    └─LlamaDecoderLayer: 3-23                     100,669,440
│    │    └─LlamaDecoderLayer: 3-24                     100,669,440
│    │    └─LlamaDecoderLayer: 3-25                     100,669,440
│    │    └─LlamaDecoderLayer: 3-26                     100,669,440
│    │    └─LlamaDecoderLayer: 3-27                     100,669,440
│    │    └─LlamaDecoderLayer: 3-28                     100,669,440
│    └─LlamaRMSNorm: 2-3                                3,072
│    └─LlamaRotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           394,002,432
================================================================================
Total params: 3,606,752,256
Trainable params: 3,606,752,256
Non-trainable params: 0
================================================================================================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForCausalLM                                        --
├─LlamaModel: 1-1                                       --
│    └─embed_tokens.weight                              ├─394,002,432
│    └─layers.0.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.0.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.0.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.0.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.0.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.0.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.0.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.0.input_layernorm.weight                  ├─3,072
│    └─layers.0.post_attention_layernorm.weight         ├─3,072
│    └─layers.1.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.1.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.1.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.1.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.1.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.1.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.1.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.1.input_layernorm.weight                  ├─3,072
│    └─layers.1.post_attention_layernorm.weight         ├─3,072
│    └─layers.2.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.2.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.2.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.2.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.2.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.2.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.2.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.2.input_layernorm.weight                  ├─3,072
│    └─layers.2.post_attention_layernorm.weight         ├─3,072
│    └─layers.3.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.3.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.3.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.3.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.3.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.3.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.3.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.3.input_layernorm.weight                  ├─3,072
│    └─layers.3.post_attention_layernorm.weight         ├─3,072
│    └─layers.4.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.4.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.4.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.4.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.4.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.4.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.4.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.4.input_layernorm.weight                  ├─3,072
│    └─layers.4.post_attention_layernorm.weight         ├─3,072
│    └─layers.5.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.5.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.5.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.5.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.5.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.5.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.5.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.5.input_layernorm.weight                  ├─3,072
│    └─layers.5.post_attention_layernorm.weight         ├─3,072
│    └─layers.6.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.6.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.6.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.6.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.6.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.6.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.6.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.6.input_layernorm.weight                  ├─3,072
│    └─layers.6.post_attention_layernorm.weight         ├─3,072
│    └─layers.7.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.7.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.7.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.7.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.7.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.7.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.7.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.7.input_layernorm.weight                  ├─3,072
│    └─layers.7.post_attention_layernorm.weight         ├─3,072
│    └─layers.8.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.8.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.8.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.8.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.8.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.8.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.8.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.8.input_layernorm.weight                  ├─3,072
│    └─layers.8.post_attention_layernorm.weight         ├─3,072
│    └─layers.9.self_attn.q_proj.weight                 ├─9,437,184
│    └─layers.9.self_attn.k_proj.weight                 ├─3,145,728
│    └─layers.9.self_attn.v_proj.weight                 ├─3,145,728
│    └─layers.9.self_attn.o_proj.weight                 ├─9,437,184
│    └─layers.9.mlp.gate_proj.weight                    ├─25,165,824
│    └─layers.9.mlp.up_proj.weight                      ├─25,165,824
│    └─layers.9.mlp.down_proj.weight                    ├─25,165,824
│    └─layers.9.input_layernorm.weight                  ├─3,072
│    └─layers.9.post_attention_layernorm.weight         ├─3,072
│    └─layers.10.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.10.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.10.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.10.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.10.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.10.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.10.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.10.input_layernorm.weight                 ├─3,072
│    └─layers.10.post_attention_layernorm.weight        ├─3,072
│    └─layers.11.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.11.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.11.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.11.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.11.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.11.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.11.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.11.input_layernorm.weight                 ├─3,072
│    └─layers.11.post_attention_layernorm.weight        ├─3,072
│    └─layers.12.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.12.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.12.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.12.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.12.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.12.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.12.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.12.input_layernorm.weight                 ├─3,072
│    └─layers.12.post_attention_layernorm.weight        ├─3,072
│    └─layers.13.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.13.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.13.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.13.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.13.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.13.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.13.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.13.input_layernorm.weight                 ├─3,072
│    └─layers.13.post_attention_layernorm.weight        ├─3,072
│    └─layers.14.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.14.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.14.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.14.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.14.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.14.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.14.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.14.input_layernorm.weight                 ├─3,072
│    └─layers.14.post_attention_layernorm.weight        ├─3,072
│    └─layers.15.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.15.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.15.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.15.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.15.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.15.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.15.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.15.input_layernorm.weight                 ├─3,072
│    └─layers.15.post_attention_layernorm.weight        ├─3,072
│    └─layers.16.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.16.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.16.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.16.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.16.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.16.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.16.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.16.input_layernorm.weight                 ├─3,072
│    └─layers.16.post_attention_layernorm.weight        ├─3,072
│    └─layers.17.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.17.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.17.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.17.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.17.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.17.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.17.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.17.input_layernorm.weight                 ├─3,072
│    └─layers.17.post_attention_layernorm.weight        ├─3,072
│    └─layers.18.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.18.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.18.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.18.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.18.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.18.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.18.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.18.input_layernorm.weight                 ├─3,072
│    └─layers.18.post_attention_layernorm.weight        ├─3,072
│    └─layers.19.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.19.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.19.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.19.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.19.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.19.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.19.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.19.input_layernorm.weight                 ├─3,072
│    └─layers.19.post_attention_layernorm.weight        ├─3,072
│    └─layers.20.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.20.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.20.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.20.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.20.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.20.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.20.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.20.input_layernorm.weight                 ├─3,072
│    └─layers.20.post_attention_layernorm.weight        ├─3,072
│    └─layers.21.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.21.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.21.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.21.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.21.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.21.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.21.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.21.input_layernorm.weight                 ├─3,072
│    └─layers.21.post_attention_layernorm.weight        ├─3,072
│    └─layers.22.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.22.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.22.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.22.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.22.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.22.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.22.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.22.input_layernorm.weight                 ├─3,072
│    └─layers.22.post_attention_layernorm.weight        ├─3,072
│    └─layers.23.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.23.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.23.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.23.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.23.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.23.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.23.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.23.input_layernorm.weight                 ├─3,072
│    └─layers.23.post_attention_layernorm.weight        ├─3,072
│    └─layers.24.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.24.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.24.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.24.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.24.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.24.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.24.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.24.input_layernorm.weight                 ├─3,072
│    └─layers.24.post_attention_layernorm.weight        ├─3,072
│    └─layers.25.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.25.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.25.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.25.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.25.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.25.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.25.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.25.input_layernorm.weight                 ├─3,072
│    └─layers.25.post_attention_layernorm.weight        ├─3,072
│    └─layers.26.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.26.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.26.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.26.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.26.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.26.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.26.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.26.input_layernorm.weight                 ├─3,072
│    └─layers.26.post_attention_layernorm.weight        ├─3,072
│    └─layers.27.self_attn.q_proj.weight                ├─9,437,184
│    └─layers.27.self_attn.k_proj.weight                ├─3,145,728
│    └─layers.27.self_attn.v_proj.weight                ├─3,145,728
│    └─layers.27.self_attn.o_proj.weight                ├─9,437,184
│    └─layers.27.mlp.gate_proj.weight                   ├─25,165,824
│    └─layers.27.mlp.up_proj.weight                     ├─25,165,824
│    └─layers.27.mlp.down_proj.weight                   ├─25,165,824
│    └─layers.27.input_layernorm.weight                 ├─3,072
│    └─layers.27.post_attention_layernorm.weight        ├─3,072
│    └─norm.weight                                      └─3,072
│    └─Embedding: 2-1                                   394,002,432
│    │    └─weight                                      └─394,002,432
│    └─ModuleList: 2-2                                  --
│    │    └─0.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─0.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─0.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─0.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─0.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─0.mlp.up_proj.weight                        ├─25,165,824
│    │    └─0.mlp.down_proj.weight                      ├─25,165,824
│    │    └─0.input_layernorm.weight                    ├─3,072
│    │    └─0.post_attention_layernorm.weight           ├─3,072
│    │    └─1.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─1.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─1.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─1.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─1.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─1.mlp.up_proj.weight                        ├─25,165,824
│    │    └─1.mlp.down_proj.weight                      ├─25,165,824
│    │    └─1.input_layernorm.weight                    ├─3,072
│    │    └─1.post_attention_layernorm.weight           ├─3,072
│    │    └─2.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─2.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─2.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─2.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─2.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─2.mlp.up_proj.weight                        ├─25,165,824
│    │    └─2.mlp.down_proj.weight                      ├─25,165,824
│    │    └─2.input_layernorm.weight                    ├─3,072
│    │    └─2.post_attention_layernorm.weight           ├─3,072
│    │    └─3.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─3.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─3.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─3.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─3.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─3.mlp.up_proj.weight                        ├─25,165,824
│    │    └─3.mlp.down_proj.weight                      ├─25,165,824
│    │    └─3.input_layernorm.weight                    ├─3,072
│    │    └─3.post_attention_layernorm.weight           ├─3,072
│    │    └─4.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─4.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─4.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─4.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─4.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─4.mlp.up_proj.weight                        ├─25,165,824
│    │    └─4.mlp.down_proj.weight                      ├─25,165,824
│    │    └─4.input_layernorm.weight                    ├─3,072
│    │    └─4.post_attention_layernorm.weight           ├─3,072
│    │    └─5.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─5.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─5.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─5.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─5.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─5.mlp.up_proj.weight                        ├─25,165,824
│    │    └─5.mlp.down_proj.weight                      ├─25,165,824
│    │    └─5.input_layernorm.weight                    ├─3,072
│    │    └─5.post_attention_layernorm.weight           ├─3,072
│    │    └─6.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─6.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─6.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─6.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─6.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─6.mlp.up_proj.weight                        ├─25,165,824
│    │    └─6.mlp.down_proj.weight                      ├─25,165,824
│    │    └─6.input_layernorm.weight                    ├─3,072
│    │    └─6.post_attention_layernorm.weight           ├─3,072
│    │    └─7.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─7.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─7.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─7.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─7.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─7.mlp.up_proj.weight                        ├─25,165,824
│    │    └─7.mlp.down_proj.weight                      ├─25,165,824
│    │    └─7.input_layernorm.weight                    ├─3,072
│    │    └─7.post_attention_layernorm.weight           ├─3,072
│    │    └─8.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─8.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─8.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─8.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─8.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─8.mlp.up_proj.weight                        ├─25,165,824
│    │    └─8.mlp.down_proj.weight                      ├─25,165,824
│    │    └─8.input_layernorm.weight                    ├─3,072
│    │    └─8.post_attention_layernorm.weight           ├─3,072
│    │    └─9.self_attn.q_proj.weight                   ├─9,437,184
│    │    └─9.self_attn.k_proj.weight                   ├─3,145,728
│    │    └─9.self_attn.v_proj.weight                   ├─3,145,728
│    │    └─9.self_attn.o_proj.weight                   ├─9,437,184
│    │    └─9.mlp.gate_proj.weight                      ├─25,165,824
│    │    └─9.mlp.up_proj.weight                        ├─25,165,824
│    │    └─9.mlp.down_proj.weight                      ├─25,165,824
│    │    └─9.input_layernorm.weight                    ├─3,072
│    │    └─9.post_attention_layernorm.weight           ├─3,072
│    │    └─10.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─10.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─10.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─10.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─10.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─10.mlp.up_proj.weight                       ├─25,165,824
│    │    └─10.mlp.down_proj.weight                     ├─25,165,824
│    │    └─10.input_layernorm.weight                   ├─3,072
│    │    └─10.post_attention_layernorm.weight          ├─3,072
│    │    └─11.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─11.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─11.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─11.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─11.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─11.mlp.up_proj.weight                       ├─25,165,824
│    │    └─11.mlp.down_proj.weight                     ├─25,165,824
│    │    └─11.input_layernorm.weight                   ├─3,072
│    │    └─11.post_attention_layernorm.weight          ├─3,072
│    │    └─12.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─12.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─12.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─12.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─12.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─12.mlp.up_proj.weight                       ├─25,165,824
│    │    └─12.mlp.down_proj.weight                     ├─25,165,824
│    │    └─12.input_layernorm.weight                   ├─3,072
│    │    └─12.post_attention_layernorm.weight          ├─3,072
│    │    └─13.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─13.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─13.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─13.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─13.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─13.mlp.up_proj.weight                       ├─25,165,824
│    │    └─13.mlp.down_proj.weight                     ├─25,165,824
│    │    └─13.input_layernorm.weight                   ├─3,072
│    │    └─13.post_attention_layernorm.weight          ├─3,072
│    │    └─14.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─14.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─14.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─14.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─14.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─14.mlp.up_proj.weight                       ├─25,165,824
│    │    └─14.mlp.down_proj.weight                     ├─25,165,824
│    │    └─14.input_layernorm.weight                   ├─3,072
│    │    └─14.post_attention_layernorm.weight          ├─3,072
│    │    └─15.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─15.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─15.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─15.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─15.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─15.mlp.up_proj.weight                       ├─25,165,824
│    │    └─15.mlp.down_proj.weight                     ├─25,165,824
│    │    └─15.input_layernorm.weight                   ├─3,072
│    │    └─15.post_attention_layernorm.weight          ├─3,072
│    │    └─16.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─16.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─16.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─16.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─16.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─16.mlp.up_proj.weight                       ├─25,165,824
│    │    └─16.mlp.down_proj.weight                     ├─25,165,824
│    │    └─16.input_layernorm.weight                   ├─3,072
│    │    └─16.post_attention_layernorm.weight          ├─3,072
│    │    └─17.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─17.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─17.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─17.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─17.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─17.mlp.up_proj.weight                       ├─25,165,824
│    │    └─17.mlp.down_proj.weight                     ├─25,165,824
│    │    └─17.input_layernorm.weight                   ├─3,072
│    │    └─17.post_attention_layernorm.weight          ├─3,072
│    │    └─18.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─18.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─18.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─18.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─18.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─18.mlp.up_proj.weight                       ├─25,165,824
│    │    └─18.mlp.down_proj.weight                     ├─25,165,824
│    │    └─18.input_layernorm.weight                   ├─3,072
│    │    └─18.post_attention_layernorm.weight          ├─3,072
│    │    └─19.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─19.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─19.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─19.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─19.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─19.mlp.up_proj.weight                       ├─25,165,824
│    │    └─19.mlp.down_proj.weight                     ├─25,165,824
│    │    └─19.input_layernorm.weight                   ├─3,072
│    │    └─19.post_attention_layernorm.weight          ├─3,072
│    │    └─20.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─20.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─20.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─20.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─20.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─20.mlp.up_proj.weight                       ├─25,165,824
│    │    └─20.mlp.down_proj.weight                     ├─25,165,824
│    │    └─20.input_layernorm.weight                   ├─3,072
│    │    └─20.post_attention_layernorm.weight          ├─3,072
│    │    └─21.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─21.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─21.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─21.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─21.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─21.mlp.up_proj.weight                       ├─25,165,824
│    │    └─21.mlp.down_proj.weight                     ├─25,165,824
│    │    └─21.input_layernorm.weight                   ├─3,072
│    │    └─21.post_attention_layernorm.weight          ├─3,072
│    │    └─22.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─22.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─22.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─22.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─22.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─22.mlp.up_proj.weight                       ├─25,165,824
│    │    └─22.mlp.down_proj.weight                     ├─25,165,824
│    │    └─22.input_layernorm.weight                   ├─3,072
│    │    └─22.post_attention_layernorm.weight          ├─3,072
│    │    └─23.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─23.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─23.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─23.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─23.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─23.mlp.up_proj.weight                       ├─25,165,824
│    │    └─23.mlp.down_proj.weight                     ├─25,165,824
│    │    └─23.input_layernorm.weight                   ├─3,072
│    │    └─23.post_attention_layernorm.weight          ├─3,072
│    │    └─24.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─24.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─24.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─24.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─24.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─24.mlp.up_proj.weight                       ├─25,165,824
│    │    └─24.mlp.down_proj.weight                     ├─25,165,824
│    │    └─24.input_layernorm.weight                   ├─3,072
│    │    └─24.post_attention_layernorm.weight          ├─3,072
│    │    └─25.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─25.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─25.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─25.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─25.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─25.mlp.up_proj.weight                       ├─25,165,824
│    │    └─25.mlp.down_proj.weight                     ├─25,165,824
│    │    └─25.input_layernorm.weight                   ├─3,072
│    │    └─25.post_attention_layernorm.weight          ├─3,072
│    │    └─26.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─26.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─26.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─26.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─26.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─26.mlp.up_proj.weight                       ├─25,165,824
│    │    └─26.mlp.down_proj.weight                     ├─25,165,824
│    │    └─26.input_layernorm.weight                   ├─3,072
│    │    └─26.post_attention_layernorm.weight          ├─3,072
│    │    └─27.self_attn.q_proj.weight                  ├─9,437,184
│    │    └─27.self_attn.k_proj.weight                  ├─3,145,728
│    │    └─27.self_attn.v_proj.weight                  ├─3,145,728
│    │    └─27.self_attn.o_proj.weight                  ├─9,437,184
│    │    └─27.mlp.gate_proj.weight                     ├─25,165,824
│    │    └─27.mlp.up_proj.weight                       ├─25,165,824
│    │    └─27.mlp.down_proj.weight                     ├─25,165,824
│    │    └─27.input_layernorm.weight                   ├─3,072
│    │    └─27.post_attention_layernorm.weight          └─3,072
│    │    └─LlamaDecoderLayer: 3-1                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-2                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-3                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-4                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-5                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-6                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-7                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-8                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-9                      100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-10                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-11                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-12                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-13                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-14                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-15                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-16                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-17                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-18                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-19                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-20                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-21                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-22                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-23                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-24                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-25                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-26                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-27                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    │    └─LlamaDecoderLayer: 3-28                     100,669,440
│    │    │    └─self_attn.q_proj.weight                ├─9,437,184
│    │    │    └─self_attn.k_proj.weight                ├─3,145,728
│    │    │    └─self_attn.v_proj.weight                ├─3,145,728
│    │    │    └─self_attn.o_proj.weight                ├─9,437,184
│    │    │    └─mlp.gate_proj.weight                   ├─25,165,824
│    │    │    └─mlp.up_proj.weight                     ├─25,165,824
│    │    │    └─mlp.down_proj.weight                   ├─25,165,824
│    │    │    └─input_layernorm.weight                 ├─3,072
│    │    │    └─post_attention_layernorm.weight        └─3,072
│    └─LlamaRMSNorm: 2-3                                3,072
│    │    └─weight                                      └─3,072
│    └─LlamaRotaryEmbedding: 2-4                        --
├─Linear: 1-2                                           394,002,432
│    └─weight                                           └─394,002,432
================================================================================
Total params: 3,606,752,256
Trainable params: 3,606,752,256
Non-trainable params: 0
================================================================================
